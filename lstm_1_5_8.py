# -*- coding: utf-8 -*-
"""LSTM_1.5.7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dKcrN76KHF8GdKt--FC0_PGtAvK0-2SG

# AUTOENCODER aug data TESTE DE VALIDAÇÃO CRUZADA
"""

# Cell for pip installing required packages
# %pip install plaidml-keras
# %pip install h5py==2.10
# %pip install 'h5py==2.10.0' --force-reinstall

# Uninstall current versions
# %pip uninstall tensorflow -y
# %pip uninstall keras -y

# Install specific versions
# %pip install tensorflow
# %pip install keras==2.1.6

# %pip install mediapipe

# Install pandas, joblib, and matplotlib
# %pip install pandas joblib matplotlib
# %pip install scikit-learn

import os
import tensorflow as tf
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import KFold
import time

start_time = time.time()

# Garantir que o TensorFlow pode ver a GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))


# Funções para carregar e preparar dados
def load_and_prepare_data(directory, window_size):
    all_frames = []
    valid_csv_count = 0  # Contador para arquivos CSV válidos

    for filename in os.listdir(directory):
        if filename.startswith('angles'):  # Alterado para 'low_pass_angles'
            df = pd.read_csv(f'{directory}/{filename}')

            # Verificar se o DataFrame não está vazio
            if not df.empty:
                valid_csv_count += 1
                # Tratamento de outliers e média móvel
                df = df.clip(lower=df.quantile(0.01), upper=df.quantile(0.99), axis=1)
                df = df.rolling(window=window_size, min_periods=1).mean()
                all_frames.append(df)

    if valid_csv_count == 0:
        print("Nenhum arquivo CSV válido encontrado.")
    else:
        print(f"Arquivos CSV válidos encontrados: {valid_csv_count}")

    return pd.concat(all_frames)

def create_time_series_data(data, n_steps):
    X = []
    for i in range(len(data) - n_steps):
        seq_x = data[i:i+n_steps]
        X.append(seq_x)
    return np.array(X)

def augment_data(data, noise_level=0.05, time_shift=3, use_noise=True, use_shift=True, reduction_factor=0.5):
    augmented_data = [data]

    if use_noise:
        # Adding noise
        noise = np.random.normal(0, noise_level, data.shape)
        data_noisy = data + noise
        augmented_data.append(data_noisy)

    if use_shift:
        # Time shifting
        data_shifted = np.roll(data, time_shift, axis=0)
        augmented_data.append(data_shifted)

    # Concatenate original and augmented data
    augmented_data = np.vstack(augmented_data)

    # Select a random subset of the augmented data
    np.random.shuffle(augmented_data)
    reduced_size = int(len(augmented_data) * reduction_factor)
    augmented_data = augmented_data[:reduced_size]

    return augmented_data

# Carregar e preparar dados
directory = 'InfinityRepVideos/armraise/csv'
window_size = 7
data = load_and_prepare_data(directory, window_size)
data_augmented = augment_data(data, use_noise=True, use_shift=False)

# Escalar dados
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data_augmented)
joblib.dump(scaler, 'InfinityRepVideos/armraise/model/autoencoder_lstm_scaler.save')

# Preparar dados para LSTM
n_steps = 48 # Número de frames por sequência
n_features = scaled_data.shape[1]
X = create_time_series_data(scaled_data, n_steps)

# Hiperparâmetros
n_units = 300 
batch_size = 48

# Construir o modelo Autoencoder LSTM
model = Sequential()

# Encoder
model.add(LSTM(units=n_units, activation='relu', kernel_regularizer=l2(0.01), input_shape=(n_steps, n_features), return_sequences=False))
model.add(Dropout(0.2))
model.add(RepeatVector(n_steps))

# Decoder
model.add(LSTM(units=n_units, activation='relu', return_sequences=True))
model.add(TimeDistributed(Dense(n_features)))

# Compilar o modelo
learning_rate = 0.0001
model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min', restore_best_weights=True)
model_checkpoint = ModelCheckpoint('InfinityRepVideos/armraise/model/26012023model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)

# Treinamento do Modelo com Validação Cruzada K-Fold
kf = KFold(n_splits=3)

train_losses = []
val_losses = []

fold = 1
for train_index, val_index in kf.split(X):
    X_train, X_val = X[train_index], X[val_index]

    print(f"Treinando Fold {fold}...")
    history = model.fit(X_train, X_train, epochs=600, batch_size=batch_size, validation_data=(X_val, X_val), callbacks=[early_stopping, model_checkpoint], verbose=1)

    val_loss = model.evaluate(X_val, X_val)
    print(f'Perda de Validação para o Fold {fold}: {val_loss}')

    fold += 1

# Após o treinamento, você pode calcular os erros de reconstrução e plotar um histograma
reconstructions = model.predict(X_val)
reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=(1, 2))  # Erros de reconstrução médios por amostra

# Lidar com valores NaN substituindo-os por zero
reconstruction_errors = np.nan_to_num(reconstruction_errors, nan=0.0)

print(f'Erro de Reconstrução Médio: {reconstruction_errors.mean()}')
print(f'Erro de Reconstrução Máximo: {reconstruction_errors.max()}')

# Plotar um histograma dos erros de reconstrução e encontrar o bin com a maior frequência
counts, bin_edges = np.histogram(reconstruction_errors, bins=50)
max_count_idx = np.argmax(counts)
peak_bin = (bin_edges[max_count_idx] + bin_edges[max_count_idx + 1]) / 2

# Plotar o histograma
plt.hist(reconstruction_errors, bins=50)
plt.title('Histograma de Erros de Reconstrução')
plt.xlabel('Erro de Reconstrução')
plt.ylabel('Frequência')

# Desenhar uma linha no pico do histograma
plt.axvline(x=peak_bin, color='r', linestyle='--', label=f'Limiar no Pico: {peak_bin:.3f}')
plt.legend()

#salvar imagem
plt.savefig('histograma_erros_reconstrucao.png')

plt.show()

# Atualizar o limiar de detecção de anomalias
anomaly_threshold = peak_bin
print(f'Limiar de Detecção de Anomalias: {anomaly_threshold}')

end_time = time.time()
total_time = end_time - start_time
print(f"Total execution time: {total_time} seconds")